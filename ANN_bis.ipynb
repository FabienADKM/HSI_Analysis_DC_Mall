{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class neuron:\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # self._parents = [0, 1, 2]   #list of neurons parents\n",
    "        # self._children = []  #list of neurons children\n",
    "        self._weight = []\n",
    "        self._input = []\n",
    "        self._output = 0\n",
    "\n",
    "\n",
    "    def _set_weight(self, weight):\n",
    "        self._weight=weight\n",
    "\n",
    "\n",
    "    def _set_input(self, input):\n",
    "        self._input=input\n",
    "        self._input.append(1)\n",
    "        self._weight = np.random.rand(1,len(input))\n",
    "        self._weight.append(1)\n",
    "\n",
    "\n",
    "    def _compute_neuron(self):\n",
    "        summed = np.dot(self._input, np.transpose(self._weight))\n",
    "        self._ouptut = sigmoid(summed)\n",
    "        return self._output\n",
    "\n",
    "\n",
    "    def _predict(self, input):\n",
    "        summed = np.dot(input, np.transpose(self._weight))\n",
    "        ouptut = sigmoid(summed)\n",
    "        return output\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_der(x):\n",
    "        return x*(1-x)\n",
    "\n",
    "\n",
    "def forward (neuron_a, Y):\n",
    "        Y_pred = neuron_a._compute_neuron()\n",
    "        loss = Y*np.log(Y_pred)\n",
    "        dw = loss*sigmoid_der(Y_pred)\n",
    "        return dw\n",
    "\n",
    "\n",
    "def optimize (neuron_a, Y, nb_iter, lr):\n",
    "    w = neuron_a._weight\n",
    "    for i in range(nb_iter):\n",
    "        dw = forward(neuron_a, Y)\n",
    "        w = w - lr * dw\n",
    "        neuron_a._weight = w\n",
    "\n",
    "\n",
    "def error_rate(y_test, y_predict):\n",
    "    sum = 0\n",
    "    n = len(y_test)\n",
    "    for i in range():\n",
    "        sum += np.abs(y_test - y_predict)\n",
    "    return sum / n\n",
    "\n",
    "\n",
    "def MSE(y_test, y_predict):\n",
    "    sum = 0\n",
    "    for i in range(len(y_test)):\n",
    "        sum += (y_test[i]-y_predict[i])**2\n",
    "    return (sum/len(y_test))\n",
    "\n",
    "\n",
    "def __main__():\n",
    "    X = []\n",
    "    cov = [[1,0],[0,1]]\n",
    "    a = 7\n",
    "    N = 300\n",
    "    x0 = np.random.multivariate_normal([-a,0], cov, int(N/2)) #Classe 0\n",
    "    x1 = np.random.multivariate_normal([0,a], cov, int(N/2)) #Classe 1\n",
    "\n",
    "    X = np.concatenate((x0, x1), axis=0)\n",
    "    y = np.zeros((N,), dtype=int)\n",
    "    y[int(N/2):N] = 1\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state= 42 )\n",
    "\n",
    "    plt.plot(X_train[:, 0], X_train[:, 1], '.')\n",
    "\n",
    "    mon_neurone = neuron()\n",
    "    mon_neurone._set_input(X_train)\n",
    "    nb_iter = 5000\n",
    "    lr = 0.05\n",
    "\n",
    "    print(\"Etienne est pd\")\n",
    "\n",
    "    optimize(neuron, y_train, nb_iter,lr)\n",
    "    y_predict = neuron._predict(X_test)\n",
    "\n",
    "    print(\"La MSE est de : \", MSE(y_test, y_predict))\n",
    "    print(\"Le taux d'erreur est de : \", error_rate(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
